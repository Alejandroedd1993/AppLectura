# ============================================
# CONFIGURACIÓN DEL BACKEND PARA RENDER
# ============================================
# Copia este archivo como .env y completa las claves
# Nota: este archivo es SOLO para el backend (server/). Para el frontend React usa .env y .env.production en la raíz.

# Environment
NODE_ENV=production

# ============================================
# CLAVES DE API - IA
# ============================================

# OpenAI (obligatorio para análisis completo)
OPENAI_API_KEY=sk-proj-tu-clave-aqui
OPENAI_MODEL=gpt-4o-mini
OPENAI_TIMEOUT=45000

# Lista de modelos permitidos en OpenAI (separados por coma).
# Útil para evitar que el cliente o una mala config fuerce modelos no deseados.
OPENAI_ALLOWED_MODELS=gpt-4o-mini

# Cap de coste para /api/analysis/text (OpenAI)
# Si no se setea, se usa 2048 por defecto.
ANALYSIS_OPENAI_MAX_TOKENS_CAP=2048

# Cap de coste para /api/analysis/text (DeepSeek)
# Si no se setea, se usa 2048 por defecto.
ANALYSIS_DEEPSEEK_MAX_TOKENS_CAP=2048

# Estrategias compuestas (control de coste)
# - smart:
#   - single (default): una sola llamada con fallback (evita pagar doble)
#   - parallel: ejecuta OpenAI + DeepSeek en paralelo (más caro)
# - debate:
#   - lite (default): 1 análisis + 1 crítica corta + merge (contraste con coste contenido)
#   - single: una sola llamada con fallback (sin crítica)
#   - parallel: ejecuta OpenAI + DeepSeek en paralelo (más caro)
ANALYSIS_SMART_MODE=single
ANALYSIS_DEBATE_MODE=lite

# Cap de tokens para la crítica en debate-lite (si no se setea, 512)
ANALYSIS_DEBATE_LITE_CRITIC_MAX_TOKENS_CAP=512

# DeepSeek (opcional, más económico para evaluaciones)
DEEPSEEK_API_KEY=sk-tu-clave-aqui
DEEPSEEK_MODEL=deepseek-chat
# Lista de modelos permitidos (separados por coma). Por seguridad, deja solo deepseek-chat.
DEEPSEEK_ALLOWED_MODELS=deepseek-chat

# Gemini (opcional)
GEMINI_API_KEY=
GEMINI_MODEL=gemini-1.5-pro

# ============================================
# BÚSQUEDA WEB (Opcional)
# ============================================

# Tavily (recomendado para búsqueda web contextual)
TAVILY_API_KEY=

# Alternativas (opcional)
SERPER_API_KEY=
BING_SEARCH_API_KEY=

# Habilita enriquecimiento web en PreLectura (default: false)
# Valores aceptados: true|1|yes|on
ENABLE_WEB_SEARCH=

# Controles de costo/latencia para PreLectura (opcionales)
# - Umbral de disparo (0..1). Default 0.4
PRELECTURA_WEB_SCORE_THRESHOLD=
# - Máximo de queries por documento. Default 3
PRELECTURA_WEB_MAX_QUERIES=
# - Resultados por query. Default 3
PRELECTURA_WEB_RESULTS_PER_QUERY=
# - Máximo de fuentes devueltas. Default 8
PRELECTURA_WEB_MAX_SOURCES=
# - Máximo de hallazgos/síntesis. Default 5
PRELECTURA_WEB_MAX_FINDINGS=

# Límite de caracteres del texto que se incluye en el prompt de PreLectura.
# Default: 18000
PRELECTURA_MAX_TEXT_CHARS=

# Timeouts y caps de coste para PreLectura (opcionales)
# - Timeout global de request/response (ms). Default: 300000
PRELECTURA_TIMEOUT_MS=
# - Safety timeout (ms) para devolver fallback antes de colgarse. Default: 295000
PRELECTURA_SAFETY_TIMEOUT_MS=
# - Cap de tokens para DeepSeek en PreLectura (máx 8000). Default: 8000
PRELECTURA_DEEPSEEK_MAX_TOKENS=
# - Timeout (ms) para DeepSeek en PreLectura. Default: 300000
PRELECTURA_DEEPSEEK_TIMEOUT_MS=
# - Cap de tokens para extracción de figuras (OpenAI) (máx 3500). Default: 3500
PRELECTURA_OPENAI_FIGURES_MAX_TOKENS=
# - Timeout (ms) para figuras (OpenAI). Default: 40000
PRELECTURA_OPENAI_FIGURES_TIMEOUT_MS=

# Modelos usados por PreLectura (forzados por allowlist)
# - DeepSeek: si se define, debe estar en DEEPSEEK_ALLOWED_MODELS
PRELECTURA_DEEPSEEK_MODEL=deepseek-chat
# - OpenAI figuras: si se define, debe estar en OPENAI_ALLOWED_MODELS
PRELECTURA_OPENAI_FIGURES_MODEL=gpt-4o-mini

# Modo aula: defaults más conservadores para evitar falsos positivos y reducir consumo
# - Default (si no seteas los límites): threshold 0.7, min indicadores 2, queries 1, resultados 2, fuentes 4
PRELECTURA_WEB_CLASSROOM_MODE=

# Caché in-memory de resultados web (reduce coste si muchos alumnos repiten texto/consultas)
# - TTL en ms. Default 300000 (5 min). 0 desactiva.
PRELECTURA_WEB_CACHE_TTL_MS=
# - Máx de entradas en cache. Default 200.
PRELECTURA_WEB_CACHE_MAX_ENTRIES=

# ============================================
# RATE LIMITING (Recomendado)
# ============================================
# Ventana en ms (default: 60000 = 1 minuto)
# Máximo de solicitudes por ventana.

# Análisis (costoso)
ANALYSIS_RATE_LIMIT_WINDOW_MS=60000
ANALYSIS_RATE_LIMIT_MAX=120

# Chat completions (costoso)
CHAT_RATE_LIMIT_WINDOW_MS=60000
CHAT_RATE_LIMIT_MAX=200

# Generación de notas (costoso)
NOTES_RATE_LIMIT_WINDOW_MS=60000
NOTES_RATE_LIMIT_MAX=60

# Búsqueda web y respuesta con IA
WEB_SEARCH_RATE_LIMIT_WINDOW_MS=60000
WEB_SEARCH_RATE_LIMIT_MAX=120

# Assessment / Evaluación (costoso)
ASSESSMENT_RATE_LIMIT_WINDOW_MS=60000
ASSESSMENT_RATE_LIMIT_MAX=10

# ============================================
# NOTAS PARA RENDER
# ============================================
# 1. NO subas este archivo con claves reales a GitHub
# 2. Configura las variables de entorno en Render Dashboard
# 3. Render establece PORT automáticamente (no lo configures manualmente)
# 4. NODE_ENV se establece en render.yaml

